Dear Dr Greaves,

I've been hitting maximal possible amounts of unproductivity this week (partially because of large movements within my country which make it quite hard to focus on other stuff; https://www.bbc.co.uk/news/world-europe-54716780). I'm writing this email to share my (quite unorganized) thoughts about the details regarding implementation of the simulation environment. I'd be very grateful if we could meet over the next few days to discuss. You're more than welcome to reply with a suggested meeting hour only and comment where appropriate then; I'm aware this email ended up being quite lengthy. Sorry!


It looks like you and dr Modi have a slightly different setup in mind. I've had a supervision with him on tuesday and it looks like he sees the simple 2d grid of processing elements which are themselves very simple units capable of doing basic ALU operations only. We had a master processing unit which moves data around and issues instructions for the other units. SIMD instructions included of course - such as "multiply data1, data2" which translates into "matrix dot_product matrix*. 

On the other hand, to me it looks like you (and initially I, too) had more of a distributed system in mind, where each processing element is a powerful independent worker (abilities similar to those of the master unit in the approach above). If we talk about a distributed system then all the things you mention such as queuing delays, connection bandwidth, etc, become important, too. 

I would be keen on implementing a simulator which would be able to work with both of these approaches, but I am not sure how to measure time.
For the master-slaves scheme it's rather simple - we can measure runtime for the master and just assume that parallel operations on slaves take some time. 

The other one is a bit more difficult. Logical clocks don't really work here, because we are interested in the estimation of physical runtime of the algorithms. We could be interested in a whole lot of different features, such as total work done, time for the slowest instance to finish, even distribution of workload, etc. Sometimes one instance issues a blocking instruction, and then we would need to synchronize the times of two threads! I believe the only sensible approach here is to measure different parameters, have a heuristic function time(parameters) estimates the time elapsed based on the parameters, and synchronizes the two (possibly update the "time_waiting" parameter to make up for the wait). Does this seem reasonable? 

Dr Modi outlines a sensible set of operations in his book which descriptions I attach in the pictures. But I still am quite puzzled on this. 


In terms of the implementation of the simulator, I think it could be a Java interface which then has certain different implementations. We can parse configuration from a config file or have functions in code to set it up too -> I prefer the latter because if we want to specify more sophisticated grid of many processing elements then the config files would grow large and we need some code to build them anyway. + then we can specify functions for time estimation, too. I would remebmer to make appropriate use of various layers of abstraction -> Simulator is the base skeleton, but there different classes such as TimeEstimator. 

For a parallel set implementation, I think it's better to wait till we want to implement Dijkstra, and for now only code a brute-force approach (or nothing at all, just specify its interface).

I've also had a look at Martin Erdos's dissertation to see how he approached the topic of simulator building (attached diss; see section 3.3) - and he took a bit different approach: each core is just a thread which can perform operations on its own data in isolation and has a message-passing primitive to send data to other cores, too. Then, he states: "By the ‘no IO’ assumption, it is sufficient to measure CPU time."
That sounds like a bit of oversimplification that we don't want, right?

I think this email is already long enough, time to stop. I hope we can chat about this soon!

Kind regards,
Kacper Walentynowicz


 
