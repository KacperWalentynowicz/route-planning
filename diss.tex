% Template for a Computer Science Tripos Part II project dissertation
\documentclass[12pt,a4paper,twoside,openright]{report}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[backend=biber]{biblatex}
\usepackage[pdfborder={0 0 0}]{hyperref}    % turns references into hyperlinks
\usepackage[margin=25mm]{geometry}  % adjusts page layout
\usepackage{graphicx}  % allows inclusion of PDF, PNG and JPG images
\usepackage{verbatim}
\usepackage{docmute}   % only needed to allow inclusion of proposal.tex
\usepackage{csquotes}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make
                                        % more readable

\addbibresource{sample.bib}

\begin{document}

%\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title


\pagestyle{empty}

\rightline{\LARGE \textbf{Kacper Walentynowicz}}

\vspace*{60mm}
\begin{center}
\Huge
\textbf{I seriously need to figure out a GOOD title} \\[5mm]
Computer Science Tripos -- Part II \\[5mm]
Trinity College \\[5mm]
\today  % today's date
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures

\pagestyle{plain}

\chapter*{Proforma}

{\large
\begin{tabular}{ll}
Name:               & \bf Kacper Walentynowicz                       \\
College:            & \bf Trinity College                     \\
Project Title:      & \bf TBA \\
Examination:        & \bf Computer Science Tripos -- Part II, July 2021  \\
Word Count:         & \bf TBA\footnotemark[1]
                      (well less than the 12000 limit)  \\
Project Originator: & Dr Jagdish Modi                    \\
Supervisors:         & Dr David Greaves, Dr Jagdish Modi                     \\ 
\end{tabular}
}
\footnotetext[1]{This word count was computed
by \texttt{detex diss.tex | tr -cd '0-9A-Za-z $\tt\backslash$n' | wc -w}
}
\stepcounter{footnote}


\section*{Original Aims of the Project}

The original aim of the project was to study several existing approaches to extending known sequential shortest path algorithms for suitability for efficient parallel computation, and to provide their implementation. Because of difficulties in evaluating parallel algorithms, a goal of the project was also to build a simulator which would enable easy implementation and fair comparison of the implemented algorithms.

\section*{Work Completed}

I have successfully implemented the tool which allows simulation
kwalentynowicz12: old algorithms were not designed with sequential model in mind, and may exhibit natural parallelism
Mar 4, 2021 3:57 PM
Hit Enter to reply
kwalentynowicz12: 1.8: symmetry, homogeneity and embedding
Mar 4, 2021 4:22 PM
Hit Enter to reply
kwalentynowicz12: Kacper, describe the claffication of parallel computers - Flynn, and more recent. Flynns, remain useful. See pages 8-9 of my book. Characteristic of the problem lends itself to SIMD or MIMD.
The cores are connected via routeing network which again plays an important role depending on the communication required in the problem itself.
Mar 10, 2021 11:13 AM
kwalentynowicz12: I will introduce Flynn's classification in preparation chapter. In introduction is too in-depth
Apr 22, 2021 10:18 AM
Hit Enter to reply
kwalentynowicz12: perhaps not the most important ones? @David
Apr 21, 2021 12:24 PM
Hit Enter to reply
kwalentynowicz12: Describe in Implementation why this feature is actually not trivial to achieve
Apr 22, 2021 3:59 PM
Hit Enter to reply
kwalentynowicz12: perhaps some better version of layout is possible here?
Apr 22, 2021 4:00 PM
Hit Enter to reply
You: This is important ......
Apr 23, 2021 11:42 AM • Edit
Hit Enter to reply
Current file
Overview
2
 of algorithms on various parallel architectures. Then, I have met the other goals of the project by implementing and evaluating several parallel adaptations of known shortest path algorithms: Dijkstra's algorithm, Matrix multiplication algorithm and SPFA algorithm. I have then compared the parallel approaches taking into account both theoretical merits and disadvantages, as well as empirical evaluation on real-world road networks.

\section*{Special Difficulties}

None.
 
\newpage
\section*{Declaration}

I, Kacper Walentynowicz of Trinity College, being a candidate for Part II of the Computer Science Tripos, hereby declare that this dissertation and the work described in it are my own work, unaided except as may be specified below, and that the dissertation does not contain material that has already been used to any substantial
extent for a comparable purpose. I am content for my dissertation to be made available to the students and staff of the University.

\bigskip
\leftline{Signed}
\vspace{20pt}
\par\noindent\rule{\textwidth}{0.4pt}

\medskip
\leftline{Date}
\vspace{20pt}
\par\noindent\rule{\textwidth}{0.4pt}

\newpage
\section*{Acknowledgements}

I am particularly thankful for the support of my two co-supervisors: Dr David Greaves and Dr Jagdish Modi. They have been helping me extensively throughout the project in different ways, both of which were valuable.

I would like to thank Dr Geoff Boeing for providing the incredibly useful OSMNX library for working with OpenStreetMap data\cite{Boeing2017}.

Last but not least, I would like to thank my wife Marta, for her insightful comments on the dissertation, and never-ending willingness to participate in one-sided discussions about the project.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% now for the chapters
\tableofcontents

\pagestyle{headings}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
How much algorithm redesign is needed for parallel architectures? It is certainly not an easy question to answer. For sequential machines, various No Free Lunch theorems \cite{free-lunch} establish that there is no algorithm perfect for all applications. This phenomenon is  visible in parallel systems where communication or synchronization costs may outweigh advantages arising from parallelization, especially if the problem instance is not large. Although there exist attempts at writing software capable of performing efficient parallelizations (such as MIT's pMapper \cite{pmapper}), designing efficient parallel algorithms remains an important challenge. In my project, I have investigated known sequential algorithms for the shortest path problem and considered their different parallel adaptations.
(DISCUSS)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Importance of parallel algorithms}
During several previous decades of computing, hardware engineers have been able to deliver better processors as quickly as Gordon Moore had anticipated in the 1970s. Recently, it seems like the boundaries can no longer be pushed as the Moore's Law has slowed down\cite{moore-law}, and programmers will need to resort to alternative ways of achieving speedup. The natural solution is using multiple workers; cores on a single machine or simply several machines communicating together. The most important benefits are energy efficiency and scalability. 

The relationship between clock frequency and used power is non-linear, and therefore to achieve the same performance a single machine needs more energy than a parallel setup. Additionally, machines are suffering from von Neumann bottleneck. The processor and memory are separate, with data moving frequently between them. Latency increasing is an unavoidable consequence of physical distance between destinations, resistance of wires, and other key factors. 

Importance of scalability also cannot be underestimated\footnote{to be precise, it is primarily a feature of distributed systems rather than all parallel ones}. The cost of adding new machines to already existing system is very cheap compared to improving processor designs. 
However, parallel systems generate numerous other difficulties which I'm discussing in section 1.3.

\section{Shortest path problem}
In my project I am studying shortest paths as an important example of fundamental computer science algorithms. Shortest paths algorithms play an important role in graph theory, also as a building block in more complicated algorithms (such as Minimum Cost Maximum Flow). Additionally, they are prevalent in other domains such as internet routing, route - planning (for vehicles), or planning in robotics. 

Formally, let $G(V,E)$ be a graph with vertex set $V$ and edge set $E$. For our purposes, each edge $i$ ($1\leq i\leq |E|)$ is of the form $(a_i, b_i, w_i)$ and allows moving from vertex $a_i$ to $b_i$ which takes time $w_i$. 

A path between two nodes ($S, T$) is therefore a sequence of edges $e_1$, $e_2$, ..., $e_k$ such that the following hold:
\begin{itemize}
    \item $a_{e_1} = S$
    \item $\displaystyle\mathop{\forall}_{2\leq i \leq k} a_{e_{i}} = b_{e_{i-1}}$
    \item $b_{e_k} = T$
\end{itemize}

Naturally, the shortest path $p_{opt}$ minimizes $\displaystyle\mathop{\sum}_{1\leq i \leq k} w_{e_i}$ among all paths $p$ which satisfy the constraints above.

There exist two main classes of traditional sequential shortest path algorithms: Single-Source Shortest Paths and All-Pairs Shortest Paths. SSSP algorithms calculate for a given node the shortest distances to all the other nodes, while APSP algorithms simply compute the distances between all possible pairs of sources and destinations simultaneously.

\subsection{Single-source shortest paths}
The most famous shortest path algorithm is arguably the Dijkstra's algorithm which is dated to late $1950$s, and described in section $2.3.2$, in the Preparation chapter. Another known shortest path algorithm is named after Bellman and Ford. It has worse theoretical complexity than Dijkstra's, but works even if the edge weights can be negative. Somewhere in the middle between these two lies a simple, yet relatively unknown Shortest Path Faster Algorithm (SPFA) described in detail in section $2.3.3$. The algorithm appears to not have been extensively studied by theoretical computer scientists, but has certain features which should allow an efficient parallel adaptation. Testing whether parallel SPFA can offer a simple and efficient solution for shortest path problems was one of the reasons which inspired me to delve into this project.

\subsection{All-pairs shortest paths}
Another approach which proves efficient especially for dense graphs is to calculate shortest paths in a way which resembles matrix multiplication. Matrix multiplication exhibits a very high degree of parallelism, and therefore this simple method may have good parallel adaptations. Alternatively, one can also consider running one of the above SSSP algorithms $|V|$ times, once for each possible source vertex.

\section{Challenges in parallelization}
Parallelization of an algorithm is not an easy task. In this section I am describing what challenges need to be addressed in order to run code efficiently in parallel.

\subsection{Importance of system architecture} 
Suppose we are doing a computationally expensive task on a system with several workers who carry out tasks. In a dream world, every worker would be able to perform their own work independently, and only report the result once they finish. Unfortunately, life is not perfect like that, and sometimes the workers need to collaborate - share data, report intermediate results, access the same resources, and so on. The costs of these operations may vary \textbf{orders} of magnitude, depending on what are the building blocks of our system. The following table shows the comparison among various possible operation costs\footnote{Author: Peter Norvig, director of research at Google. Table taken from the article: '\textit{Teach yourself programming in 10 years}'}.
\begin{center}
\includegraphics[scale=0.4]{norvig.png}
\end{center}

When talking about classical sequential algorithms these costs are often neglected. In parallel systems communication of workers and their utilization are crucial for efficiency, and therefore need to be carefully considered.

\subsection{Sequential mode of operation}
Some algorithms (such as Dijkstra's) are notoriously difficult to parallelize due to their sequential nature of operation -- later steps depend on earlier steps. A parallel adaptation of a highly efficient sequential algorithm may be truly disappointing if workers are idle majority of the time. On the other hand, simple algorithms may exhibit natural parallelism. Old algorithms which predate computing era are especially promising, as they were not designed with execution on serial machines in mind. One of those is the aforementioned SPFA.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Preparation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Starting point}
Prior to the project, I did not have much experience with parallel programming. Some principles and methods have been outlined in \textit{Concurrent and Distributed Systems} part IB course.

Starting point of the project, was therefore, nearly from level zero. Apart from reading shallowly about possible parallelizations of Dijkstra's algorithm. The only code which I had implemented, were sequential versions of Dijkstra's and SPFA algorithms to serve as a proof-of-concept that SPFA can compete with Dijkstra's algorithm in terms of performance even in the standard setup of sequential machines\footnote{See the beginning of $2.3.3$ for details}.

\section{Requirements analysis}
As described in the introduction, parallel algorithms are an interesting, but challenging topic. Because performance of algorithms is heavily influenced by system architecture, my project needs to consider these. Following initial analysis of the problem, I have decided that a key component of the project would be a simulator providing an evaluation environment for the algorithms.

The tool needed the following features:

\begin{enumerate}
    \item Allow the abstract description of a parallel algorithm to be easily translated into a working code.
    \item Provide functionalities for cores - entities which are going to perform work of our algorithms. These include basic primitives such as ALU\footnote{Arithmetic Logic Unit} operations, accessing local DRAM of the cores, as well as simple data structures for those: local arrays, queues, and sets. 
    \item Provide means of communicating for the cores - either by passing messages (broadcast or core-core), or accessing shared data structures in memory.
    \item Allow specifying the topology of interconnections to reflect that in real-world machines there is limited number of direct connection between workers.
    \item All of the operations should be implicitly tracked by the evaluation environment. The programmer should also be able to specify what the costs of various operations in the system are \footnote{they are independent of the algorithm used for computation}, in order to test the algorithms in different circumstances.
    \item Running the simulations using multiple threads of a physical machine (my laptop with an $8$-core processor), so that the computations do not run unreasonably long.
    \item Allowing to run the simulations in single-threaded debug mode. There are lots of surprising concurrency bugs which can arise from incorrect implementations and are not easy to find. Therefore, a debug mode which allows deterministic execution could prove very useful.
\end{enumerate}

\section{Algorithms}
In my project I have studied parallelizations of three different standard algorithms for shortest paths, and evaluated their relative tradeoffs. In this section I am briefly describing the sequential versions and the ideas associated with their efficient parallelizations. The exact division between \textit{Preparation} and \textit{Implementation} chapters is somewhat blurred, as understanding the algorithm, its exact requirements and limitations is the key part. Afterwards, translating the algorithm into the code running on the simulation environment should be rather smooth.

In the descriptions of the algorithms I am assuming the standard conventions: the graph $G(V,E)$ has $N$ vertices labeled from $1$ to $N$ and $M$ edges of non-negative weights. If a path between the source does not exist, its length is $\infty$. In parallel algorithms, workers (called interchangeably 'cores') are labeled from $1$ to $C$.

\subsection{Matrix multiplication}
The first algorithm implemented by me was an algorithm based on matrix multiplication, let's call it MatMul. It is an All-Pairs shortest paths algorithm which calculates the answer in the array $dist[N][N]$. Its operation consists of $log(N)$ steps of dynamic programming.

In the initialization phase, we calculate the shortest distance assuming that we are allowed to traverse no more than $1$ edge - the adjacency matrix.

Now, let's assume we want to know what are the shortest paths if we are allowed to traverse no more than $2K$ edges, for some $K$. What does a path from $a$ to $b$ that uses no more than $2K$ edges consist of? A path from $a$ to some intermediary node $c$ of no more than $K$ edges, followed by a path from $c$ to $b$, also of no more than $K$ edges. We don't know what $c$ is, but we can simply consider all the options! 

Therefore, given $dist_{K}[N][N]$ we can easily calculate $dist_{2K}[N][N]$. The following pseudocode solves our shortest path problem.

\begin{algorithm}
\caption{MatMulDynamicProgramming}\label{matmuldp}
\begin{algorithmic}[1]
\Procedure{MatMulDP(G)}{}
\State $\textit{dist} \gets G.adj\_matrix()$
\State $\textit{newdist} \gets dist$
    \For{$step \gets 1$ to $log(N)$} 
        \For{$a \gets 1$ to $N$}
        \For{$b \gets 1$ to $N$}
            \State $newdist[a][b] \gets dist[a][b]$
        \For{$c \gets 1$ to $N$}
            \State $newdist[a][b] \gets min(newdist[a][b], dist[a][c] + dist[c][b])$
        \EndFor
        \EndFor
        \EndFor
        \State $dist \gets newdist$
      \EndFor
\State \textbf{return} $dist$
\EndProcedure
\end{algorithmic}
\end{algorithm}

One can notice that the innermost loop resembles matrix multiplications with an operator switch - sum over all elements is substituted with min, and we are adding two terms rather than multiplying.

\begin{algorithm}
\caption{MatMul}\label{matmul}
\begin{algorithmic}[1]
\Procedure{MatMul(G)}{}
\State $\textit{dist} \gets G.adj\_matrix()$
    \For{$step \gets 1$ to $log(N)$}
        \State $dist \gets multiply\_matrix\_by\_itself(dist,(+,min))$
      \EndFor
\State \textbf{return} $dist$
\EndProcedure
\end{algorithmic}
\end{algorithm}

It is very clear what needs to be done to obtain an efficient parallel algorithm - write good matrix multiplication! For that, we can use Cannon's algorithm \cite{cannon}, which ensures that there is little unnecessary movement of data between iterations. The operation of Cannon's algorithm is described in the \textit{Implementation} section of MatMul, which would be almost empty if I included a description of Cannon's algorithm here. 

\subsection{Dijkstra's algorithm}
Dijkstra's algorithm is one of the most well known computer science algorithms taught in every course on standard algorithms, and therefore just recalling its pseudocode will suffice.

\begin{algorithm}
\caption{Dijkstra's algorithm}\label{dijkstra}
\begin{algorithmic}[1]
\Procedure{Dijkstra(G, source)}{}
\For{$node \gets 1$ to $N$}
    \State $dist[node] \gets \infty$
\EndFor
\State $dist[source] \gets 0$
\State $Q \gets G.nodes()$
    \While{\textbf{not} Q.empty()}
        \State $u \gets$ node in $Q$ with minimal $dist[]$
        \State $Q \gets Q \setminus \{u\}$
        \For{\textbf{each} edge ($u$, $v$, $w$)}
        \If{$dist[v] > dist[u] + w$}
            \State $dist[v] \gets dist[u] + w$
        \EndIf
      \EndFor
    \EndWhile
    
    
\State \textbf{return} $dist$
\EndProcedure
\end{algorithmic}
\end{algorithm}

There is a notable difficulty in parallelizing this algorithm - in line $7$ nodes are being taken one after another in different iterations. There are lots of data dependencies, because changes in $dist$ may affect which nodes in the subsequent iterations. Nonetheless, there have been attempts at efficient parallelizations.

$Q$ is a priority queue, typically implemented with a Fibonacci heap to guarantee $O(|E| + |V| log |V|)$ runtime. A simple approach for a parallel speedup would be to split the slow computation of selecting the vertex with minimal distance. In the initialization phase, we would like to partition the nodes for our workers, so that each node has exactly one worker assigned to it. Therefore, $assigned(u)$ be a function which returns the worker to whom node $u$ was assigned. Whenever line $7$ is executed, each of the workers would calculate minimal distance vertex independently, and afterwards they would communicate to find the global minimal distance vertex.

There is a subtlety here: when a successful relaxation of edge $(u,v,w)$ is performed in line $11$, the worker $assigned(v)$ must be made aware of this change. This can be resolved either by sending a message from $assigned(u)$ to $assigned(v)$, or by $assigned(v)$ periodically scanning through distances of vertices it is responsible for rather than keeping them in a priority queue. Let's summarize the approach in a pseudocode.

\begin{algorithm}
\caption{Simple Parallel Dijkstra}\label{pdijkstra}
\begin{algorithmic}[1]
\Procedure{SimpleParallelDijkstra(G, source)}{}
\For{$node \gets 1$ to $N$}
    \State $dist[node] \gets \infty$
\EndFor
\State $dist[source] \gets 0$
\State $assign\_nodes\_to\_workers()$
\State $Q \gets G.nodes()$
    \While{\textbf{not} Q.empty()}
        \For{\textbf{each} core $c$ \textbf{concurrently}}
        \State $u_c \gets$ node in $Q$ such that $assigned(u_c) = c$ with minimal $dist[]$
        \EndFor
        \State $u \gets u_c$ with minimal $dist[]$
        \State $Q \gets Q \setminus \{u\}$
        \For{\textbf{each} edge ($u$, $v$, $w$)}
        \If{$dist[v] > dist[u] + w$}
            \State $dist[v] \gets dist[u] + w$
            \State $communicate\_change(assigned(v), v, dist[u] + w)$
        \EndIf
      \EndFor
    \EndWhile
    
    
\State \textbf{return} $dist$
\EndProcedure
\end{algorithmic}
\end{algorithm}

Crauser et al. (1998) \cite{dijkstra} introduced a strong improvement. In their paper \textit{A Parallelization of Dijkstra's Shortest Path Algorithm} they describe another opportunity for exploiting parallelism. The idea is to run the algorithm in phases, and in each phase try to remove from $Q$ many vertices whose distance is never going to change.\footnote{unlike just one in the standard algorithm}

Let $OUT(u)$ be the minimal weight of an edge going out of $u$. If $dist[u]$ is guaranteed not to change, all vertices $v$ satisfying $dist[v] \leq dist[u] + OUT(u)$ can never be relaxed by $u$. At the beginning of each phase, we can calculate $L = min(dist[u] + OUT(u))$ among vertices $u$ present in $Q$. Then, all vertices $u$ satisfying $dist[u] \leq L$ can be removed from $Q$ simultaneously!\footnote{As $L$ is $min(dist[u] + OUT(u))$ over all nodes in $Q$, no node can relax any further}

Crauser et al. (1998)\cite{dijkstra} prove that this optimization reduces with high probability the number of phases to $\sqrt{|V|}$ while staying work-efficient. In section $4$ of their paper, the parallel implementation is also suggested. I leave the details to the \textit{Implementation} chapter.

\subsection{Shortest Path Faster Algorithm}
Although one can extract a lot of parallelism from such a highly sequential algorithm like Dijkstra's, it may be better to design an algorithm with parallelism in mind. 

A limiting factor of Dijkstra's algorithm is the strong constraint that a vertex may never be relaxed again, and maintaining a priority queue. A relatively unpopular algorithm which originated in the $1950$s\footnote{According to Wikipedia, the exact description (with a name algorithm D) can be found in:  Moore, Edward F. (1959).  ”The shortest path through a maze”.  Proceedings of the International Symposium on the Theory of Switching.  Harvard University Press.  pp.  285–292. I have not managed to find the access to this resource and verify this information myself.} lacks these bottlenecks. A simple version of this algorithm abandons priority in the queue and performs relaxations while there are some possible.

\begin{algorithm}
\caption{Chaotic relaxation}\label{chaotic}
\begin{algorithmic}[1]
\Procedure{Chaotic Relaxation(G, source)}{}
\For{$node \gets 1$ to $N$}
    \State $dist[node] \gets \infty$
\EndFor
\State $dist[source] \gets 0$
\State $Q \gets \{source\}$ \Comment{Q is a normal queue without priorities}
    \While{\textbf{not} Q.empty()}
        \State $u \gets Q.pop\_front()$
        \For{\textbf{each} edge ($u$, $v$, $w$)}
        \If{$dist[v] > dist[u] + w$}
            \State $dist[v] \gets dist[u] + w$
            \State $Q.push(v)$
        \EndIf
      \EndFor
    \EndWhile
\State \textbf{return} $dist$
\EndProcedure
\end{algorithmic}
\end{algorithm}

Correctness of the algorithm is obvious - the relaxations are going to be performed as long as there is one. SPFA adds a tiny change in line $11$: it does not add a vertex to the queue if it is there already.  

\begin{algorithm}
\caption{SPFA}\label{spfa}
\begin{algorithmic}[1]
\Procedure{SPFA(G, source)}{}
\For{$node \gets 1$ to $N$}
    \State $dist[node] \gets \infty$
\EndFor
\State $dist[source] \gets 0$
\State $Q \gets \{source\}$ \Comment{Q is a normal queue without priorities}
    \While{\textbf{not} Q.empty()}
        \State $u \gets Q.pop\_front()$
        \For{\textbf{each} edge ($u$, $v$, $w$)}
        \If{$dist[v] > dist[u] + w$}
            \State $dist[v] \gets dist[u] + w$
            \If{$v$ is not in $Q$} \Comment{The only change from ChaoticRelaxation}
                \State $Q.push(v)$
                \EndIf
        \EndIf
      \EndFor
    \EndWhile
\State \textbf{return} $dist$
\EndProcedure
\end{algorithmic}
\end{algorithm}

This tiny change gives an empirically fast algorithm, which is believed to work in $O(|E|)$ on random graphs. In recent years, SPFA has been popularized by fans of algorithmic competitions\footnote{and this is how I have heard about it first}, and it even has a Wikipedia page\footnote{https:\/\/en.wikipedia.org\/wiki\/Shortest\_Path\_Faster\_Algorithm}, but sources there appear to be several niche websites citing one another. Analyzing mathematically the expected runtime of SPFA is outside the scope of this project. Nonetheless, before the project I have carried out experiments to check behaviour of this algorithm on a large graph - California Road Network collected by various researches and introduced in \cite{CRN}. Nodes were put into the queue $7$ times on average, and the runtime was faster than Dijkstra's algorithm's. The initial experiments showed that not only was SPFA fast, but also promising in terms of parallel execution. This is because there are little restrictions on the order in which relaxations should be performed, which makes it suitable for parallel implementation.

Devising a parallel version is very natural, and uses a similar mapping of graph nodes to cores as parallel Dijkstra. 

\begin{algorithm}
\caption{Parallel SPFA}\label{pspfa}
\begin{algorithmic}[1]
\Procedure{Parallel SPFA(G, source)}{}
\For{$node \gets 1$ to $N$}
    \State $dist[node] \gets \infty$
\EndFor
\State $dist[source] \gets 0$
\State $assign\_nodes\_to\_workers()$
\State $Q \gets \{source\}$ \Comment{Q is a normal queue without priorities}
    \While{\textbf{not} Q.empty()}
        \For{\textbf{each} core $c$ \textbf{concurrently}}
        \State $u_c \gets Q.pop\_front()$
        \For{\textbf{each} edge ($u_c$, $v$, $w$)}
        \If{$dist[v] > dist[u_c] + w$}
            \State $dist[v] \gets dist[u_c] + w$
            \If{$v$ is not in $Q$}
                \State $Q.push(v)$
                \EndIf
        \EndIf
        \EndFor
      \EndFor
    \EndWhile
\State \textbf{return} $dist$
\EndProcedure
\end{algorithmic}
\end{algorithm}

There are several concurrency issues possible: $Q$ needs to be accessed by multiple workers, and collaboration on shared $dist$ array is required. The discussion is continued in the \textit{Implementation} chapter.

\section{Reconstruction of paths}
In section $2.3$ the algorithms calculate only the array of shortest distances, rather than all the paths. This simplifies the codes and explanations. To give a full picture, this short section explains how to extend the algorithms to answer queries about the paths.

We are going to introduce a predecessor array $pred[]$ which will maintain previous node on the shortest path. For Dijkstra's algorithm and SPFA it is sufficient to update $pred[]$ alongside with relaxations: when we are applying an update $dist[v] = dist[u] + w$, we also set $pred[v] = u$.

In case of MatMul the change is marginal, too. For initialization:$\displaystyle\mathop{\forall}_{a,b}pred[a][b] = a$. For the second phase, $min(newdist[a][b],dist[a][c]+dist[c][b])$ can be rewritten equivalently, after which it's clear when to update predecessor matrix:
\begin{algorithm}
\begin{algorithmic}[1]
    \If{$newdist[a][b] > dist[a][c] + dist[c][b]$}
        \State $newdist[a][b] \gets dist[a][c] + dist[c][b]$
        \State $pred[a][b] \gets pred[c][b]$
    \EndIf
\end{algorithmic}
\end{algorithm}
Starting this way would not allow me to draw a connection betwen the algorithm and matrix multiplication so clearly.

There is another, even easier approach which can be useful especially if tight memory constraints are an issue: calculating the predecessor on the spot, just based on the distance array, as in the following code.

\begin{algorithm}
\caption{Reconstruction of paths}\label{pathrec}
\begin{algorithmic}[1]
\Procedure{Reconstruct-Path(source, dest)}{}
\If{source $=$ dest}
\State \textbf{return} [dest]
\Else
    \State $pred \gets None$
    \For{\textbf{each} edge ($u$, $dest$, $w$)}
    \If{$dist[u] + w = dist[dest]$}
    \State $pred \gets u$
    \EndIf
    \EndFor
    \State \textbf{return} append(Reconstruct-Path(source, pred), dest)
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

Once line $9$ is reached, $pred$ cannot be $None$ - there must exist a previous vertex on the shortest path, and that vertex satisfies $dist[u] + w = dist[dest]$. This concludes the remark that with little additional effort, we can answer any shortest path query given array of distances.

\section{Datasets}
To test shortest path algorithms, one needs graphs. I have decided to get my data with the help of OSMNX\cite{Boeing2017}\footnote{Although the author does not mention it explicitly, the name comes probably from crossing OpenStreetMap with NetworkX, which on its own explains a lot about this library}. This python package, created by Geoff Boeing, is an excellent tool for working with road networks. It is a perfect fit for my project, because it provides one-line functions for:
\begin{itemize}
    \item downloading road networks of any size based on geographical locations or names
    \item plotting networks or shortest paths
    \item automatic simplification of network topologies by removing graph nodes with $2$ neighbors
\end{itemize} 

Let's finish the chapter with a demonstration of OSMNX's usefulness. Creating the plot below took me only a couple of minutes.

\includegraphics[scale=0.6]{code-demo-osmnx.png}
\begin{center}\includegraphics[scale=0.5]{camb-demo-osmnx.png}\end{center}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Implementation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Software Development practices}
here I describe what practices I intended to follow: why Java, why python for plotting and osmnx, frequent commit policy, backups on Google Drive

\section{Simulator}

\subsection{Overview}
addressing the requirements, high-level ideas - message passing, primitives, etc. CLEARLY describe which assumptions are taken.

\subsection{Description of building blocks}
I describe what building blocks are there in my simulator, and which are hidden from the outside world but important

\subsubsection{Tasks and Phases}
Task as a main building block. No coroutines, so this solution.

Naturally occurring in algorithms - dependencies between phases, cannot start running phase N+1 until N is finished.

\subsubsection{Scheduler and task runners}
Like a thread-pool. Abstracting away from number of threads used in our algorithms. Hidden from the outside world. 

\subsubsection{Cores and processor architectures}
Arbitrarily powerful entities. Every operation is executed by some core.

\subsubsection{Trackers and Estimators}
Track operations and calculate metrics. Easy changes - just supply a different Estimator class if you want different penalties (addressing requirements... again!).

\subsubsection{Evaluation environment}
Describes the class env, and how the components interact with each other as a system


\section{Parallel matrix multiplication}
I'm thinking whether moving challenges to this section here would make more sense... 
how I addressed the challenges to implement parallel matrix multiplication

\section{Parallel Dijkstra's algorithm}
there are many many challenges here, how can we implement this efficiently

\section{Parallel SPFA implementation}
what is done and why it is done like that

\section{Code testing strategy}
What I am doing to ensure that my algorithms are correct: cross-validation against each other and against networkx libraries

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Evaluation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Theoretical complexities}
This is going to be SHORT. Mat mul is trivial, dijkstra is super complicated and SPFA is unproven... Is it even necessary? 

\section{Execution times}
Plots related solely to execution times go here. Will try to write models and fit with scikit-learn.

\section{Energy efficiency}
Is the algorithm energy-efficient? Energy usage, power consumption, estimated as total work performed among cores.

\section{Sensitivity analysis}
In this chapter I'm going to perform perturbations of various parameters and calculate how the results change to be able to confidently report results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Well... I don't know what the conclusions are going to be! It would be good to say that everything was evaluated successfully, and we demonstrated some interesting results... 

\section{Future work}
What else interesting can be done

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography
\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the appendices
\appendix

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\chapter{Latex source}

%\section{diss.tex}
%{\scriptsize\verbatiminput{diss.tex}}

%\section{proposal.tex}
%{\scriptsize\verbatiminput{proposal.tex}}

%\chapter{Makefile}

%\section{makefile}\label{makefile}
%{\scriptsize\verbatiminput{makefile.txt}}

%\section{refs.bib}
%{\scriptsize\verbatiminput{refs.bib}}

\chapter{Project Proposal}

%\input{proposal}

\end{document}
