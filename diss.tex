% Template for a Computer Science Tripos Part II project dissertation
\documentclass[12pt,a4paper,twoside,openright]{report}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[backend=biber]{biblatex}
\usepackage[pdfborder={0 0 0}]{hyperref}    % turns
\usepackage[table,xcdraw]{xcolor}
\usepackage[margin=25mm]{geometry}  % adjusts page layout
\usepackage{courier} %% Sets font for listing as Courier.
\usepackage{listings, xcolor}
\usepackage{graphicx}  % allows inclusion of PDF, PNG and JPG images
\usepackage{verbatim}
\usepackage{docmute}   % only needed to allow inclusion of proposal.tex
\usepackage{csquotes}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{dirtree}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make
                                        % more readable

\addbibresource{sample.bib}

\lstset{
tabsize = 4, %% Sets tab space width.
showstringspaces = false, %% Prevents space marking in strings, string is defined as the text that is generally printed directly to the console.
numbers = left, %% Displays line numbers on the left.
commentstyle = \color{green}, %% Sets comment color.
keywordstyle = \color{blue}, %% Sets  keyword color.
stringstyle = \color{red}, %% Sets  string color.
rulecolor = \color{black}, %% Sets frame color to avoid being affected by text color.
basicstyle = \small \ttfamily , %% Sets listing font and size.
breaklines = true, %% Enables line breaking.
numberstyle = \tiny,
}

\begin{document}

%\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title


\pagestyle{empty}

\rightline{\LARGE \textbf{Kacper Walentynowicz}}

\vspace*{60mm}
\begin{center}
\Huge
\textbf{I seriously need to figure out a GOOD title} \\[5mm]
Computer Science Tripos -- Part II \\[5mm]
Trinity College \\[5mm]
\today  % today's date
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures

\pagestyle{plain}

\chapter*{Proforma}

{\large
\begin{tabular}{ll}
Name:               & \bf Kacper Walentynowicz                       \\
College:            & \bf Trinity College                     \\
Project Title:      & \bf TBA \\
Examination:        & \bf Computer Science Tripos -- Part II, July 2021  \\
Word Count:         & \bf TBA\footnotemark[1]
                      (well less than the 12000 limit)  \\
Project Originator: & Dr Jagdish Modi                    \\
Supervisors:         & Dr David Greaves, Dr Jagdish Modi                     \\ 
\end{tabular}
}
\footnotetext[1]{This word count was computed
by \texttt{detex diss.tex | tr -cd '0-9A-Za-z $\tt\backslash$n' | wc -w}
}
\stepcounter{footnote}


\section*{Original Aims of the Project}

The original aim of the project was to study several existing approaches to extending known sequential shortest path algorithms for suitability for efficient parallel computation, and to provide their implementation. Because of difficulties in evaluating parallel algorithms, a goal of the project was also to build a simulator which would enable easy implementation and fair comparison of the implemented algorithms.

\section*{Work Completed}

I have successfully implemented the tool which allows simulation of algorithms on various parallel architectures. Then, I have met the other goals of the project by implementing and evaluating several parallel adaptations of known shortest path algorithms: Dijkstra's algorithm, Matrix multiplication algorithm and SPFA algorithm. I have then compared the parallel approaches taking into account both theoretical merits and disadvantages, as well as empirical evaluation on real-world road networks.

\section*{Special Difficulties}

None.
 
\newpage
\section*{Declaration}

I, Kacper Walentynowicz of Trinity College, being a candidate for Part II of the Computer Science Tripos, hereby declare that this dissertation and the work described in it are my own work, unaided except as may be specified below, and that the dissertation does not contain material that has already been used to any substantial
extent for a comparable purpose. I am content for my dissertation to be made available to the students and staff of the University.

\bigskip
\leftline{Signed}
\vspace{20pt}
\par\noindent\rule{\textwidth}{0.4pt}

\medskip
\leftline{Date}
\vspace{20pt}
\par\noindent\rule{\textwidth}{0.4pt}

\newpage
\section*{Acknowledgements}

I am particularly thankful for the support of my two co-supervisors: Dr David Greaves and Dr Jagdish Modi. They have been helping me extensively throughout the project in different ways, both of which were valuable.

I would like to thank Dr Geoff Boeing for providing the incredibly useful OSMNX library for working with OpenStreetMap data\cite{Boeing2017}.

Last but not least, I would like to thank my wife Marta, for her insightful comments on the dissertation, and never-ending willingness to participate in one-sided discussions about the project.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% now for the chapters
\tableofcontents

\pagestyle{headings}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
How much algorithm redesign is needed for parallel architectures? It is certainly not an easy question to answer. For sequential machines, various No Free Lunch theorems \cite{free-lunch} establish that there is no algorithm perfect for all applications. This phenomenon is  visible in parallel systems where communication or synchronization costs may outweigh advantages arising from parallelization, especially if the problem instance is not large. Although there exist attempts at writing software capable of performing efficient parallelizations (such as MIT's pMapper \cite{pmapper}), designing efficient parallel algorithms remains an important challenge. In my project, I have investigated known sequential algorithms for the shortest path problem and considered their different parallel adaptations.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Importance of parallel algorithms}
During several previous decades of computing, hardware engineers have been able to deliver better processors as quickly as Gordon Moore had anticipated in the 1970s. Recently, it seems like the boundaries can no longer be pushed as the Moore's Law has slowed down\cite{moore-law}, and programmers will need to resort to alternative ways of achieving speedup. The natural solution is using multiple workers; cores on a single machine or simply several machines communicating together. The most important benefits are energy efficiency and scalability. 

The relationship between clock frequency and used power is non-linear, and therefore to achieve the same performance a single machine needs more energy than a parallel setup. Additionally, machines are suffering from von Neumann bottleneck. The processor and memory are separate, with data moving frequently between them. Latency increasing is an unavoidable consequence of physical distance between destinations, resistance of wires, and other key factors. 

Importance of scalability also cannot be underestimated\footnote{to be precise, it is primarily a feature of distributed systems rather than all parallel ones}. The cost of adding new machines to already existing system is very cheap compared to improving processor designs. 
However, parallel systems generate numerous other difficulties which I'm discussing in section 1.3.

\section{Shortest path problem}
In my project I am studying shortest paths as an important example of fundamental computer science algorithms. Shortest paths algorithms play an important role in graph theory, also as a building block in more complicated algorithms (such as Minimum Cost Maximum Flow). Additionally, they are prevalent in other domains such as internet routing, route - planning (for vehicles), or planning in robotics. 

Formally, let $G(V,E)$ be a graph with vertex set $V$ and edge set $E$. For our purposes, each edge $i$ ($1\leq i\leq |E|)$ is of the form $(a_i, b_i, w_i)$ and allows moving from vertex $a_i$ to $b_i$ which takes time $w_i$. 

A path between two nodes ($S, T$) is therefore a sequence of edges $e_1$, $e_2$, ..., $e_k$ such that the following hold:
\begin{itemize}
    \item $a_{e_1} = S$
    \item $\displaystyle\mathop{\forall}_{2\leq i \leq k} a_{e_{i}} = b_{e_{i-1}}$
    \item $b_{e_k} = T$
\end{itemize}

Naturally, the shortest path $p_{opt}$ minimizes $\displaystyle\mathop{\sum}_{1\leq i \leq k} w_{e_i}$ among all paths $p$ which satisfy the constraints above.

There exist two main classes of traditional sequential shortest path algorithms: Single-Source Shortest Paths and All-Pairs Shortest Paths. SSSP algorithms calculate for a given node the shortest distances to all the other nodes, while APSP algorithms simply compute the distances between all possible pairs of sources and destinations simultaneously.

\subsection{Single-source shortest paths}
The most famous shortest path algorithm is arguably the Dijkstra's algorithm which is dated to late $1950$s, and described in section $2.3.2$, in the Preparation chapter. Another known shortest path algorithm is named after Bellman and Ford. It has worse theoretical complexity than Dijkstra's, but works even if the edge weights can be negative. Somewhere in the middle between these two lies a simple, yet relatively unknown Shortest Path Faster Algorithm (SPFA) described in detail in section $2.3.3$. The algorithm appears to not have been extensively studied by theoretical computer scientists, but has certain features which should allow an efficient parallel adaptation. Testing whether parallel SPFA can offer a simple and efficient solution for shortest path problems was one of the reasons which inspired me to delve into this project.

\subsection{All-pairs shortest paths}
Another approach which proves efficient especially for dense graphs is to calculate shortest paths in a way which resembles matrix multiplication. Matrix multiplication exhibits a very high degree of parallelism, and therefore this simple method may have good parallel adaptations. Alternatively, one can also consider running one of the above SSSP algorithms $|V|$ times, once for each possible source vertex.

\section{Challenges in parallelization}
Parallelization of an algorithm is not an easy task. In this section I am describing what challenges need to be addressed in order to run code efficiently in parallel.

\subsection{Importance of system architecture} 
Suppose we are doing a computationally expensive task on a system with several workers who carry out tasks. In a dream world, every worker would be able to perform their own work independently, and only report the result once they finish. Unfortunately, life is not perfect like that, and sometimes the workers need to collaborate - share data, report intermediate results, access the same resources, and so on. The costs of these operations may vary \textbf{orders} of magnitude, depending on what are the building blocks of our system. The following table shows the comparison among various possible operation costs\footnote{Author: Peter Norvig, director of research at Google. Table taken from the article: '\textit{Teach yourself programming in 10 years}'}.
\begin{center}
\includegraphics[scale=0.4]{norvig.png}
\end{center}

When talking about classical sequential algorithms these costs are often neglected. In parallel systems communication of workers and their utilization are crucial for efficiency, and therefore need to be carefully considered.

\subsection{Sequential mode of operation}
Some algorithms (such as Dijkstra's) are notoriously difficult to parallelize due to their sequential nature of operation -- later steps depend on earlier steps. A parallel adaptation of a highly efficient sequential algorithm may be truly disappointing if workers are idle majority of the time. On the other hand, simple algorithms may exhibit natural parallelism. Old algorithms which predate computing era are especially promising, as they were not designed with execution on serial machines in mind. One of those is the aforementioned SPFA.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Preparation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Starting point}
Prior to the project, I did not have much experience with parallel programming. Some principles and methods have been outlined in \textit{Concurrent and Distributed Systems} part IB course.

Starting point of the project, was therefore, nearly from level zero. Apart from reading shallowly about possible parallelizations of Dijkstra's algorithm. The only code which I had implemented, were sequential versions of Dijkstra's and SPFA algorithms to serve as a proof-of-concept that SPFA can compete with Dijkstra's algorithm in terms of performance even in the standard setup of sequential machines\footnote{See the section $2.3.3$ for details}.

\section{Requirements analysis}
As described in the introduction, parallel algorithms are an interesting, but challenging topic. Because performance of algorithms is heavily influenced by system architecture, my project needs to consider these. Following initial analysis of the problem, I have decided that a key component of the project would be a simulator providing an evaluation environment for the algorithms.

The tool needed the following features:

\begin{enumerate}
    \item Allow the abstract description of a parallel algorithm to be easily translated into a working code.
    \item Provide functionalities for cores - entities which are going to perform work of our algorithms. These include basic primitives such as ALU\footnote{Arithmetic Logic Unit} operations, accessing local DRAM of the cores, as well as simple data structures for those: local arrays, queues, and sets. 
    \item Provide means of communicating for the cores - either by passing messages (broadcast or core-core), or accessing shared data structures in memory.
    \item Allow specifying the topology of interconnections to reflect that in real-world machines there is limited number of direct connection between workers.
    \item All of the operations should be implicitly tracked by the evaluation environment. The programmer should also be able to specify what the costs of various operations in the system are \footnote{they are independent of the algorithm used for computation}, in order to test the algorithms in different circumstances.
    \item Running the simulations using multiple threads of a physical machine (my laptop with an $8$-core processor), so that the computations do not run unreasonably long.
    \item Allowing to run the simulations in single-threaded debug mode. There are lots of surprising concurrency bugs which can arise from incorrect implementations and are not easy to find. Therefore, a debug mode which allows deterministic execution could prove very useful.
\end{enumerate}

\section{Algorithms}
In my project I have studied parallelizations of three different standard algorithms for shortest paths, and evaluated their relative tradeoffs. In this section I am briefly describing the sequential versions and the ideas associated with their efficient parallelizations. The exact division between \textit{Preparation} and \textit{Implementation} chapters is somewhat blurred, as understanding the algorithm, its exact requirements and limitations is the key part. Afterwards, translating the algorithm into the code running on the simulation environment should be rather smooth.

In the descriptions of the algorithms I am assuming the standard conventions: the graph $G(V,E)$ has $N$ vertices labeled from $1$ to $N$ and $M$ edges of non-negative weights. If a path between the source does not exist, its length is $\infty$. In parallel algorithms, workers (called interchangeably 'cores') are labeled from $1$ to $C$.

\subsection{Matrix multiplication}
The first algorithm implemented by me was an algorithm based on matrix multiplication, let's call it MatMul. It is an All-Pairs shortest paths algorithm which calculates the answer in the array $dist[N][N]$. Its operation consists of $log(N)$ steps of dynamic programming.

In the initialization phase, we calculate the shortest distance assuming that we are allowed to traverse no more than $1$ edge - the adjacency matrix.

Now, let's assume we want to know what are the shortest paths if we are allowed to traverse no more than $2K$ edges, for some $K$. What does a path from $a$ to $b$ that uses no more than $2K$ edges consist of? A path from $a$ to some intermediary node $c$ of no more than $K$ edges, followed by a path from $c$ to $b$, also of no more than $K$ edges. We don't know what $c$ is, but we can simply consider all the options! 

Therefore, given $dist_{K}[N][N]$ we can easily calculate $dist_{2K}[N][N]$. The following pseudocode solves our shortest path problem.

\begin{algorithm}
\caption{MatMulDynamicProgramming}\label{matmuldp}
\begin{algorithmic}[1]
\Procedure{MatMulDP(G)}{}
\State $\textit{dist} \gets G.adj\_matrix()$
\State $\textit{newdist} \gets dist$
    \For{$step \gets 1$ to $log(N)$} 
        \For{$a \gets 1$ to $N$}
        \For{$b \gets 1$ to $N$}
            \State $newdist[a][b] \gets dist[a][b]$
        \For{$c \gets 1$ to $N$}
            \State $newdist[a][b] \gets min(newdist[a][b], dist[a][c] + dist[c][b])$
        \EndFor
        \EndFor
        \EndFor
        \State $dist \gets newdist$
      \EndFor
\State \textbf{return} $dist$
\EndProcedure
\end{algorithmic}
\end{algorithm}

One can notice that the innermost loop resembles matrix multiplications with an operator switch - sum over all elements is substituted with min, and we are adding two terms rather than multiplying.

\begin{algorithm}
\caption{MatMul}\label{matmul}
\begin{algorithmic}[1]
\Procedure{MatMul(G)}{}
\State $\textit{dist} \gets G.adj\_matrix()$
    \For{$step \gets 1$ to $log(N)$}
        \State $dist \gets multiply\_matrix\_by\_itself(dist,(+,min))$
      \EndFor
\State \textbf{return} $dist$
\EndProcedure
\end{algorithmic}
\end{algorithm}

It is very clear what needs to be done to obtain an efficient parallel algorithm - write good matrix multiplication! For that, we can use Cannon's algorithm \cite{cannon}, which ensures that there is little unnecessary movement of data between iterations. The operation of Cannon's algorithm is described in the \textit{Implementation} section of MatMul, which would be almost empty if I included a description of Cannon's algorithm here. 

\subsection{Dijkstra's algorithm}
Dijkstra's algorithm is one of the most well known computer science algorithms taught in every course on standard algorithms, and therefore just recalling its pseudocode will suffice.

\begin{algorithm}
\caption{Dijkstra's algorithm}\label{dijkstra}
\begin{algorithmic}[1]
\Procedure{Dijkstra(G, source)}{}
\For{$node \gets 1$ to $N$}
    \State $dist[node] \gets \infty$
\EndFor
\State $dist[source] \gets 0$
\State $Q \gets G.nodes()$
    \While{\textbf{not} Q.empty()}
        \State $u \gets$ node in $Q$ with minimal $dist[]$
        \State $Q \gets Q \setminus \{u\}$
        \For{\textbf{each} edge ($u$, $v$, $w$)}
        \If{$dist[v] > dist[u] + w$}
            \State $dist[v] \gets dist[u] + w$
        \EndIf
      \EndFor
    \EndWhile
    
    
\State \textbf{return} $dist$
\EndProcedure
\end{algorithmic}
\end{algorithm}

There is a notable difficulty in parallelizing this algorithm - in line $7$ nodes are being taken one after another in different iterations. There are lots of data dependencies, because changes in $dist$ may affect which nodes in the subsequent iterations. Nonetheless, there have been attempts at efficient parallelizations.

$Q$ is a priority queue, typically implemented with a Fibonacci heap to guarantee $O(|E| + |V| log |V|)$ runtime. A simple approach for a parallel speedup would be to split the slow computation of selecting the vertex with minimal distance. In the initialization phase, we would like to partition the nodes for our workers, so that each node has exactly one worker assigned to it. Let, $assigned(u)$ be a function which returns the worker to whom node $u$ was assigned. Whenever line $7$ is executed, each of the workers would calculate minimal distance vertex independently, and afterwards they would communicate to find the global minimal distance vertex.

There is a subtlety here: when a successful relaxation of edge $(u,v,w)$ is performed in line $11$, the worker $assigned(v)$ must be made aware of this change. This can be resolved either by sending a message from $assigned(u)$ to $assigned(v)$, or by $assigned(v)$ periodically scanning through distances of vertices it is responsible for rather than keeping them in a priority queue. Let's summarize the approach in a pseudocode.

\begin{algorithm}
\caption{Simple Parallel Dijkstra}\label{pdijkstra}
\begin{algorithmic}[1]
\Procedure{SimpleParallelDijkstra(G, source)}{}
\For{$node \gets 1$ to $N$}
    \State $dist[node] \gets \infty$
\EndFor
\State $dist[source] \gets 0$
\State $assign\_nodes\_to\_workers()$
\State $Q \gets G.nodes()$
    \While{\textbf{not} Q.empty()}
        \For{\textbf{each} core $c$ \textbf{concurrently}}
        \State $u_c \gets$ node in $Q$ such that $assigned(u_c) = c$ with minimal $dist[]$
        \EndFor
        \State $u \gets u_c$ with minimal $dist[]$
        \State $Q \gets Q \setminus \{u\}$
        \For{\textbf{each} edge ($u$, $v$, $w$)}
        \If{$dist[v] > dist[u] + w$}
            \State $dist[v] \gets dist[u] + w$
            \State $communicate\_change(assigned(v), v, dist[u] + w)$
        \EndIf
      \EndFor
    \EndWhile
    
    
\State \textbf{return} $dist$
\EndProcedure
\end{algorithmic}
\end{algorithm}

Crauser et al. (1998) \cite{dijkstra} introduced a strong improvement. In their paper \textit{A Parallelization of Dijkstra's Shortest Path Algorithm} they describe another opportunity for exploiting parallelism. The idea is to run the algorithm in phases, and in each phase try to remove from $Q$ many vertices whose distance is never going to change.\footnote{unlike just one in the standard algorithm}

Let $OUT(u)$ be the minimal weight of an edge going out of $u$. If $dist[u]$ is guaranteed not to change, all vertices $v$ satisfying $dist[v] \leq dist[u] + OUT(u)$ can never be relaxed by $u$. At the beginning of each phase, we can calculate $L = min(dist[u] + OUT(u))$ among vertices $u$ present in $Q$. Then, all vertices $u$ satisfying $dist[u] \leq L$ can be removed from $Q$ simultaneously!\footnote{As $L$ is $min(dist[u] + OUT(u))$ over all nodes in $Q$, no node can relax any further}

Crauser et al. (1998)\cite{dijkstra} prove that this optimization reduces with high probability the number of phases to $\sqrt{|V|}$ while staying work-efficient. In section $4$ of their paper, the parallel implementation is also suggested. I leave the details to the \textit{Implementation} chapter.

\subsection{Shortest Path Faster Algorithm}
Although one can extract a lot of parallelism from such a highly sequential algorithm like Dijkstra's, it may be better to design an algorithm with parallelism in mind. 

Two most limiting factors of Dijkstra's algorithm are: the strong constraint that a vertex may never be relaxed again, and maintaining a priority queue. A relatively unpopular algorithm which originated in the $1950$s\footnote{According to Wikipedia, the exact description (with a name algorithm D) can be found in:  Moore, Edward F. (1959).  ”The shortest path through a maze”.  Proceedings of the International Symposium on the Theory of Switching.  Harvard University Press.  pp.  285–292. I have not managed to find the access to this resource and verify this information myself.} lacks these bottlenecks. A simple version of this algorithm abandons priority in the queue and performs relaxations while there are some possible.

\begin{algorithm}
\caption{Chaotic relaxation}\label{chaotic}
\begin{algorithmic}[1]
\Procedure{Chaotic Relaxation(G, source)}{}
\For{$node \gets 1$ to $N$}
    \State $dist[node] \gets \infty$
\EndFor
\State $dist[source] \gets 0$
\State $Q \gets \{source\}$ \Comment{Q is a normal queue without priorities}
    \While{\textbf{not} Q.empty()}
        \State $u \gets Q.pop\_front()$
        \For{\textbf{each} edge ($u$, $v$, $w$)}
        \If{$dist[v] > dist[u] + w$}
            \State $dist[v] \gets dist[u] + w$
            \State $Q.push(v)$
        \EndIf
      \EndFor
    \EndWhile
\State \textbf{return} $dist$
\EndProcedure
\end{algorithmic}
\end{algorithm}

Correctness of the algorithm is obvious - the relaxations are going to be performed as long as there is one. SPFA adds a tiny change in line $11$: it does not add a vertex to the queue if it is there already.  

\begin{algorithm}
\caption{SPFA}\label{spfa}
\begin{algorithmic}[1]
\Procedure{SPFA(G, source)}{}
\For{$node \gets 1$ to $N$}
    \State $dist[node] \gets \infty$
\EndFor
\State $dist[source] \gets 0$
\State $Q \gets \{source\}$ \Comment{Q is a normal queue without priorities}
    \While{\textbf{not} Q.empty()}
        \State $u \gets Q.pop\_front()$
        \For{\textbf{each} edge ($u$, $v$, $w$)}
        \If{$dist[v] > dist[u] + w$}
            \State $dist[v] \gets dist[u] + w$
            \If{$v$ is not in $Q$} \Comment{The only change from ChaoticRelaxation}
                \State $Q.push(v)$
                \EndIf
        \EndIf
      \EndFor
    \EndWhile
\State \textbf{return} $dist$
\EndProcedure
\end{algorithmic}
\end{algorithm}

This tiny change gives an empirically fast algorithm, which is believed to work in $O(|E|)$ on random graphs. In recent years, SPFA has been popularized by fans of algorithmic competitions\footnote{and this is how I have heard about it first}, and it even has a Wikipedia page\footnote{https://en.wikipedia.org/wiki/Shortest\_Path\_Faster\_Algorithm}, but sources there appear to be several niche websites citing one another. Analyzing mathematically the expected runtime of SPFA is outside the scope of this project. Nonetheless, before the project I have carried out experiments to check behaviour of this algorithm on a large graph - California Road Network collected by various researches and introduced in \cite{CRN}. Nodes were put into the queue $7$ times on average, and the runtime was faster than Dijkstra's algorithm's. The initial experiments showed that not only was SPFA fast, but also promising in terms of parallel execution. This is because there are little restrictions on the order in which relaxations should be performed, which makes it suitable for parallel implementation.

Devising a parallel version is very natural, and uses a similar mapping of graph nodes to cores as parallel Dijkstra. 

\begin{algorithm}
\caption{Parallel SPFA}\label{pspfa}
\begin{algorithmic}[1]
\Procedure{Parallel SPFA(G, source)}{}
\For{$node \gets 1$ to $N$}
    \State $dist[node] \gets \infty$
\EndFor
\State $dist[source] \gets 0$
\State $assign\_nodes\_to\_workers()$
\State $Q \gets \{source\}$ \Comment{Q is a normal queue without priorities}
    \While{\textbf{not} Q.empty()}
        \For{\textbf{each} core $c$ \textbf{concurrently}}
        \State $u_c \gets Q.pop\_front()$
        \For{\textbf{each} edge ($u_c$, $v$, $w$)}
        \If{$dist[v] > dist[u_c] + w$}
            \State $dist[v] \gets dist[u_c] + w$
            \If{$v$ is not in $Q$}
                \State $Q.push(v)$
                \EndIf
        \EndIf
        \EndFor
      \EndFor
    \EndWhile
\State \textbf{return} $dist$
\EndProcedure
\end{algorithmic}
\end{algorithm}

There are several concurrency issues possible: $Q$ needs to be accessed by multiple workers, and collaboration on shared $dist$ array is required. The discussion is continued in the \textit{Implementation} chapter.

\section{Reconstruction of paths}
In section $2.3$ the algorithms calculate only the array of shortest distances, rather than all the paths. This simplifies the codes and explanations. To give a full picture, this short section explains how to extend the algorithms to answer queries about the paths.

We are going to introduce a predecessor array $pred[]$ which will maintain previous node on the shortest path. For Dijkstra's algorithm and SPFA it is sufficient to update $pred[]$ alongside with relaxations: when we are applying an update $dist[v] = dist[u] + w$, we also set $pred[v] = u$.

In case of MatMul the change is marginal, too. For initialization:$\displaystyle\mathop{\forall}_{a,b}pred[a][b] = a$. For the second phase, $min(newdist[a][b],dist[a][c]+dist[c][b])$ can be rewritten equivalently, after which it's clear when to update predecessor matrix:

\begin{algorithm}
\begin{algorithmic}[1]
    \If{$newdist[a][b] > dist[a][c] + dist[c][b]$}
        \State $newdist[a][b] \gets dist[a][c] + dist[c][b]$
        \State $pred[a][b] \gets pred[c][b]$
    \EndIf
\end{algorithmic}
\end{algorithm}
Starting this way would not allow me to draw a connection betwen the algorithm and matrix multiplication so clearly.

There is another, even easier approach which can be useful especially if tight memory constraints are an issue: calculating the predecessor on the spot, just based on the distance array, as in the following code.

\begin{algorithm}
\caption{Reconstruction of paths}\label{pathrec}
\begin{algorithmic}[1]
\Procedure{Reconstruct-Path(source, dest)}{}
\If{source $=$ dest}
\State \textbf{return} [dest]
\Else
    \State $pred \gets None$
    \For{\textbf{each} edge ($u$, $dest$, $w$)}
    \If{$dist[u] + w = dist[dest]$}
    \State $pred \gets u$
    \EndIf
    \EndFor
    \State \textbf{return} append(Reconstruct-Path(source, pred), dest)
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

Once line $9$ is reached, $pred$ cannot be $None$ - there must exist a previous vertex on the shortest path, and that vertex satisfies $dist[u] + w = dist[dest]$. This concludes the remark that with little additional effort, we can answer any shortest path query given array of distances.

\section{Datasets}
To test shortest path algorithms, one needs graphs. I have decided to get my data with the help of OSMNX\cite{Boeing2017}\footnote{Although the author does not mention it explicitly, the name comes probably from crossing OpenStreetMap with NetworkX, which on its own explains a lot about this library}. This python package, created by Geoff Boeing, is an excellent tool for working with road networks. It is a perfect fit for my project, because it provides one-line functions for:
\begin{itemize}
    \item downloading road networks of any size based on geographical locations or names
    \item plotting networks or shortest paths
    \item automatic simplification of network topologies by removing graph nodes with $2$ neighbors
\end{itemize} 

Let's finish the chapter with a demonstration of OSMNX's usefulness. Creating the plot below took me only a couple of minutes.

\includegraphics[scale=0.6]{code-demo-osmnx.png}
\begin{center}\includegraphics[scale=0.5]{camb-demo-osmnx.png}\end{center}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Implementation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Software Development - practices and tools}
My project consisted of two parts - developing the simulator to run the algorithms, and coding the actual parallel algorithms in the simulator. I consider my work to have been done in a spiral-like method of software development. Although I had tried to carefully design the simulator by anticipating features needed by the algorithms, there were a couple bits which needed modifications. Additionally, algorithms themselves allowed incremental improvements - initially do a part of code in a suboptimal, but easier way, then fix the shortcomings in the later versions.  

The natural choice for implementing the project was Java. The language has excellent support for writing concurrent (I have used \textit{java.util.concurrent} extensively throughout the project) and object-oriented programs. It also had the pluses of being taught in Part IA and IB courses, so I didn't need to spend time learning a new language for the project.

But Java also has drawbacks, and I couldn't imagine doing evaluation in Java. The results of running the algorithms were produced in the form of CSV files, so that I could then import them in Python easily. Python was a much better choice for doing data manipulations, comparisons, plotting. I had to use Python anyway if I wanted to make use of the aforementioned OSMnx module (I don't think that its ports to other languages exist). Altogether, I have written the entire project in Java with the exception of two Python scripts: \textit{graph\_download.py} and \textit{evaluation.py}. I have also made heavy use of the JetBrains IDEs (IntelliJ and PyCharm). I especially appreciated their capabilities for automatically resolving dependencies, imports and refactorings with usage search. Obeying builtin linter suggestions made me confident that the code follows good stylistic conventions.

Throughout the project I have been meeting regularly with my supervisors. Usually had a meeting once a week with either Dr Modi (Tuesdays), or Dr Greaves (Wednesdays). Additionally, during Michealmas and Lent terms my Directors of Studies held short ($10$ minutes) update meetings, where I would present recent developments in the project. When doing the work itself, I tried to follow the division suggested in my project proposal - biweekly sprints, but a few changes arose. The simulator work took longer than I had anticipated (partially because I made it much more sophisticated than originally planned), but later I caught up as the work on algorithms went faster than anticipated. I have also switched the order of implementing parallel Dijkstra and MatMul algorithms, as the latter seemed less  error-prone.  

I have also made use of tools with version-control systems to ensure that nothing gets lost. The code written in my GitHub repository was frequently committed and pushed, so that the changes persist even in case of hardware failures. For writing my dissertation I have used Overleaf - a free online \LaTeX editor which allows real-time preview and collaboration on the documents, annotating the text with pop-up comments and even features a version control system. Although I have been using the tool for many years and I generally trust Overleaf's cloud, the dissertation is a good moment to become paranoid - so I have been doing additional backups daily on my machine and pushing the latest version to GitHub to be 101\% sure of not losing the work.

\section{Repository overview}
Let's start with the figure which gives a directory structure of my project. Crucially, the diagram does not mention every single Java class. I have followed a modular approach where classes have single responsibilities. The diagram is much clearer if I include just example files. A full diagram is provided as Appendix A. Every piece source code present in that diagram was written by me, except for \textit{FibonacciHeap.java} whose author is Keith Schwarz\footnote{htiek@cs.stanford.edu}. Throughout the chapter I am going to describe what various building blocks are, what assumptions they take, and how they interact with each other.

\dirtree{%
.1 route-planning.
.2 data.
.3 cambridge.graphml.
.3 cambridge.txt.
.2 src.
.3 algorithms.
.4 APSP.
.5 MatMul.
.6 CannonMatMul.java.
.4 SSSP.
.5 SSSPAlgorithm.java.
.5 Dijkstra.
.6 Dijkstra.java.
.6 DijkstraCore.java.
.5 SPFA.
.6 SPFA.java.
.6 SPFACore.java.
.3 graphs.
.4 graph\_download.ipynb.
.4 Graph.java.
.3 simulator.
.4 internal.
.5 TaskRunner.java.
.5 Scheduler.java.
.4 utils.
.5 Message.java.
.5 EvaluationEnvironment.java.
.5 LocalArray.java.
.2 test.
.3 DijkstraMultiThreadedTests.java .
.3 evaluation.ipynb.
}

\section{Simulator}
\subsection{Overview}
The simulator provides an evaluation environment for the algorithms to run on. In this section I am describing its implementation, as well as the functions it provides for coding algorithms. Looking from a very high-level perspective, a parallel system consists of cores executing various operations provided. Additionally, a master core is introduced which is helpful for doing bookkeeping work.
The algorithms are expressed in threaded code\footnote{not to be confused with multithreading} - they consist almost exclusively of calls to subroutines provided. Each core has access to its own local memory. One assumption taken is that local memories are insufficient to hold the entire graphs, but the graphs fit the total memory of the cores (rather than streamed through some external storage). Therefore, the cores need to communicate their data to other cores. They can do it through passing messages. Let's now proceed to describing the building blocks of the simulator.

\subsection{Tasks and Phases}
Since the early days of computing, algorithms are expressed as sequences of phases in which certain tasks are done. In the simulator \texttt{Tasks} are classes for chunks of work to be done. They have a method \texttt{execute()} which allows to specify the operations to be done for the core, as in the example which uses the provided simple matrix multiplication subroutine to multiply two matrices stored in local memory of some core. 

\begin{lstlisting}[language = Java , frame = trBL , firstnumber = last , escapeinside={(*@}{@*)}]
Task multiply_matrices = new Task(myCore) {
    @Override
    public void execute() {
        myCore.simpleMult(myCore.c, myCore.a, myCore.b);
    }
};
\end{lstlisting}


\texttt{Tasks} are grouped into \texttt{Phases}. Under the hood, \texttt{Phase} class has a thread-safe queue of Tasks and almost nothing more.

Every sensible system should provide guarantees when effects of certain operations are visible - it's unreasonable to assume they occur immediately. I have chosen the bulk-synchronous model: the tasks cannot assume anything about effects of Tasks from the same phase, but they can assume that effects of Tasks from previous phases have already occurred. An example consequence is that when cores need to pass some data cyclically, sending the data should be done in one phase and receiving the data in the next phase.

Additionally, there is a functionality for a task to be repeated. This feature is added to support types of algorithms which operate until there are no changes.

One subtlety arose in the design process. The single-threaded debug mode would deadlock if any of the Tasks contained operations was blocking. I have investigated the possibility of using a Java package for coroutines to allow the execution to be suspended and resumed, but decided that adding it would be overly complicated for the needs and sticked to the assumption that Tasks execute non-blocking methods which run to completion (but they may, of course, in parallel with other tasks). In hindsight, this design decision did not turn out constraining.

\subsection{Scheduler and task runners}
The important requirement was that the algorithms are described in terms of operations on abstract cores. \texttt{Scheduler} and \texttt{Task runners} are responsible of handling that and running the computation on the physical machine. The \texttt{Task runner} class extends a java Thread and provides an ability to run tasks from a phases. \texttt{Scheduler} spawns these runners and manages them to ensure that the environment does not deadlock. Once all the tasks from a phase are executed, the Scheduler instructs the runners to finish polling new tasks from this phase and switch to the next one instead (or issues the command for runners to terminate).

The asynchronous execution posed some challenges. Runners need to poll new tasks from the phase in a non-blocking way. Initially I just borrowed my own implementation of a queue done in practical 3 of FurtherJava course last year, but ultimately I've changed it to a  \texttt{ConcurrentLinkedQueue} from \texttt{java.util.concurrent} package, as it had the desired behaviour. Once all the running threads receive null tasks in the same phase, the phase is finished.

The operation of scheduler and task runners follows closely the thread pool design pattern. Summing up, the implementation is hidden in \texttt{simulator/internal}; the programmer just needs to specify an appropriate number of threads. For my machine the optimal number is $8$, but better computers may benefit from using more threads. 

\subsection{Cores and processor architectures}
Arbitrarily powerful entities. Every operation is executed by some core.

\subsection{Trackers and Estimators}
Track operations and calculate metrics. Easy changes - just supply a different Estimator class if you want different penalties (addressing requirements... again!).

\subsection{Evaluation environment}
Describes the class env, and how the components interact with each other as a system


\section{Parallel matrix multiplication}
I'm thinking whether moving challenges to this section here would make more sense... 
how I addressed the challenges to implement parallel matrix multiplication

$\begin{bmatrix}
A_{00} \boxtimes B_{00} & A_{01} \boxtimes B_{11} & A_{02} \boxtimes B_{22} \\
A_{11} \boxtimes B_{10} & A_{12} \boxtimes B_{21} & A_{10} \boxtimes B_{02} \\
A_{22} \boxtimes B_{20} & A_{20} \boxtimes B_{01} & A_{21} \boxtimes B_{12} \\
\end{bmatrix}$

$\begin{bmatrix}
A_{01} \boxtimes B_{10} & A_{02} \boxtimes B_{21} & A_{00} \boxtimes B_{02} \\
A_{12} \boxtimes B_{20} & A_{10} \boxtimes B_{01} & A_{11} \boxtimes B_{12} \\
A_{20} \boxtimes B_{00} & A_{21} \boxtimes B_{11} & A_{22} \boxtimes B_{22} \\
\end{bmatrix}$

$\begin{bmatrix}
A_{02} \boxtimes B_{20} & A_{00} \boxtimes B_{01} & A_{01} \boxtimes B_{12} \\
A_{10} \boxtimes B_{00} & A_{11} \boxtimes B_{11} & A_{12} \boxtimes B_{22} \\
A_{21} \boxtimes B_{10} & A_{22} \boxtimes B_{21} & A_{20} \boxtimes B_{02} \\
\end{bmatrix}$

\section{Parallel Dijkstra's algorithm}
there are many many challenges here, how can we implement this efficiently

\section{Parallel SPFA implementation}
what is done and why it is done like that

\section{Code testing strategy}
What I am doing to ensure that my algorithms are correct: cross-validation against each other and against networkx libraries

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Evaluation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Theoretical complexities}
This is going to be SHORT. Mat mul is trivial, dijkstra is super complicated and SPFA is unproven... Is it even necessary? 

\section{Execution times}
Plots related solely to execution times go here. Will try to write models and fit with scikit-learn.

\section{Energy efficiency}
Is the algorithm energy-efficient? Energy usage, power consumption, estimated as total work performed among cores.

\section{Sensitivity analysis}
In this chapter I'm going to perform perturbations of various parameters and calculate how the results change to be able to confidently report results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Well... I don't know what the conclusions are going to be! It would be good to say that everything was evaluated successfully, and we demonstrated some interesting results... 

\section{Future work}
What else interesting can be done

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography
\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the appendices
\appendix

\chapter{Detailed repository overview diagram}
\dirtree{%
.1 route-planning.
 .2 data.
 .2 src.
  .3 algorithms.
   .4 APSP.MatMul.
    .5 CannonCore.java.
    .5 CannonMatMul.java.
    .5 MatMulAlgorithm.java.
    .5 MatMulPolicy.java.
   .4 SSSP.
   .4 Dijkstra.
    .5 Dijkstra.java.
    .5 DijkstraCore.java.
    .5 DijkstraNoSpeedup.java.
   .4 SPFA.
    .5 SPFA.java.
    .5 SPFACore.java.
    .5 SPFANoSpeedup.java.
   .4 SSAlgorithm.java.
   .4 SSPolicy.java.
  .3 graphs.
   .4 cache.
    .5 Assignment.java.
    .5 Edge.java.
    .5 evaluation.ipynb.
    .5 Graph.java.
    .5 graph\_download.ipynb.
    .5 Matrix.java.
   .4 simulator.
    .5 internal.
     .6 CommunicationHandler.java.
     .6 ConcurrentQueue.java.
     .6 ConcurrentQueueImpl.java.
     .6 Scheduler.java.
     .6 SingleConnectionHandler.java.
     .6 TaskRunner.java.
    .5 utils.
     .6 Core.java.
     .6 Estimator.java.
     .6 EvaluationEnvironment.java.
     .6 FibonacciHeap.java.
     .6 GlobalArray.java.
     .6 GlobalBitset.java.
     .6 GlobalQueue.java.
     .6 Lattice.java.
     .6 LocalArray.java.
     .6 LocalHeap.java.
     .6 MasterCore.java.
     .6 Message.java.
     .6 Metrics.java.
     .6 OpTracker.java.
     .6 Phase.java.
     .6 ProcessorArchitecture.java.
     .6 Task.java.
 .2 test.
  .3 DijkstraMultiThreadedTests.java.
  .3 DijkstraNoSpeedupMultiThreadedTests.java.
  .3 DijkstraNoSpeedupSingleThreadedTests.java.
  .3 Evaluation.java.
  .3 MatMulAlgorithmMultiThreadedTests.java.
  .3 MatMulAlgorithmSingleThreadedTests.java.
  .3 SPFAMultiThreadedTests.java.
  .3 SPFANoSpeedupMultiThreadedTests.java.
}

\noindent Directory \texttt{data/} contains data generated automatically generated with help of OSMnx, but it would be pointless to include those.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\chapter{Latex source}

%\section{diss.tex}
%{\scriptsize\verbatiminput{diss.tex}}

%\section{proposal.tex}
%{\scriptsize\verbatiminput{proposal.tex}}

%\chapter{Makefile}

%\section{makefile}\label{makefile}
%{\scriptsize\verbatiminput{makefile.txt}}

%\section{refs.bib}
%{\scriptsize\verbatiminput{refs.bib}}

\chapter{Project Proposal}

%\input{proposal}

\end{document}